## Lexer

this is the forefront of our compiler toolset, the first phase of a compiler (or an interpreter) that takes the source
code (written by the programmer) as a raw string and converts it into a sequence of meaningful units called `tokens`.

this process is also called `Lexical Analysis`, `Scanning`, or `Tokenizing`.